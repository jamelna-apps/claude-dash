{
  "version": "1.0",
  "lastUpdated": "2026-01-17T00:00:00Z",

  "use": [
    {
      "item": "Native apps over Docker for GPU-intensive workloads",
      "reason": "Metal GPU acceleration not available in Docker on Mac",
      "examples": ["Ollama", "ML inference"],
      "addedOn": "2026-01-13"
    },
    {
      "item": "AnythingLLM for local AI chat and document RAG",
      "reason": "Single tool for chat + RAG, API accessible via claude-dash gateway",
      "addedOn": "2026-01-13"
    },
    {
      "item": "Task-based model routing for optimal performance on M2 16GB",
      "reason": "Specialized models for each task type: deepseek-coder (code), gemma3:4b (RAG/128K context), phi3:mini (speed), qwen3-vl (vision)",
      "models": {
        "code": "deepseek-coder:6.7b",
        "rag": "gemma3:4b",
        "quick": "phi3:mini",
        "vision": "qwen3-vl:8b",
        "embeddings": "nomic-embed-text"
      },
      "addedOn": "2026-01-17"
    }
  ],

  "avoid": [
    {
      "item": "Running LLMs in Docker on Mac",
      "reason": "No Metal GPU = 40x slower inference",
      "alternative": "Use native Ollama installation",
      "addedOn": "2026-01-13"
    },
    {
      "item": "Redundant tools with overlapping functionality",
      "reason": "Simplicity, less maintenance, fewer resources",
      "example": "Removed Open WebUI (AnythingLLM covers same use cases)",
      "addedOn": "2026-01-13"
    }
  ],

  "conventions": [
    {
      "topic": "role hierarchy and decision making",
      "rule": "User is ultimate decision maker (Product Owner/Tech Lead). Claude Sonnet is Senior Developer (leads technical design, architecture, coding). Local Ollama models are SUPPORT TOOLS (research assistants, code reviewers, quick helpers). Claude makes confident technical recommendations and codes directly - local models assist but don't replace Claude's expertise.",
      "roles": {
        "user": "Final decision maker, product requirements, feature approval",
        "claude_sonnet": "Senior developer - architecture, design, complex coding, technical leadership",
        "deepseek_coder": "Code review assistant - second opinion on code quality",
        "gemma3_4b": "Research assistant - RAG, codebase exploration (128K context)",
        "phi3_mini": "Quick helper - commit messages, simple docs",
        "qwen3_vl": "UI specialist - screenshot analysis, design review"
      },
      "workflow": "Claude designs and codes. Local models support with: research (gemma3 RAG), code review (deepseek), quick tasks (phi3), UI analysis (qwen3-vl). Claude remains senior developer - models are junior assistants.",
      "priority": "critical",
      "addedOn": "2026-01-17"
    },
    {
      "topic": "claude workflow",
      "rule": "Use MCP memory tools (memory_query, memory_search, memory_functions) for instant lookups. Use MLX CLI (mlx rag, mlx review) as SUPPORT TOOLS - for research and code review assistance, not as replacement for Claude's technical leadership.",
      "priority": "critical",
      "addedOn": "2025-12-15",
      "lastUpdated": "2026-01-17"
    },
    {
      "topic": "model selection for agents",
      "rule": "Use gradient approach: MLX local ($0) -> Haiku ($) -> Sonnet ($$) -> Opus ($$$$). Reserve Opus for genuinely complex architecture only (~25% of planning). Use Sonnet for 75% of planning and ALL implementation.",
      "decision_tree": {
        "planning": {
          "complex": "Opus - Greenfield architecture, multiple non-obvious approaches, high reversal cost, cross-cutting concerns",
          "standard": "Sonnet - Following patterns, clear approach, well-understood problems, refactoring (75% of cases)"
        },
        "implementation": "Sonnet - ALWAYS use Sonnet, even for Opus-planned features",
        "exploration": "Haiku - Fast file/code exploration",
        "quick_tasks": "Haiku - Formatting, simple refactors, debug logging, documentation"
      },
      "examples": [
        "Task(model: 'opus') ONLY for genuinely complex architecture (e.g., 'design offline-first sync system')",
        "Task(model: 'sonnet') for standard planning (e.g., 'plan user profile screen')",
        "Task(model: 'sonnet') for ALL implementation/coding",
        "Task(model: 'haiku') for exploration, simple tasks"
      ],
      "priority": "critical",
      "addedOn": "2025-12-18",
      "lastUpdated": "2026-01-10"
    },
    {
      "topic": "ollama local ai tools",
      "rule": "Use NATIVE Ollama (Metal GPU) with MLX tools PROACTIVELY to save tokens. Automatic task routing: deepseek-coder (code), gemma3:4b (RAG/128K), phi3:mini (fast), qwen3-vl (vision). Use for RAG queries, code review, error analysis. Also use doc_query for personal document searches.",
      "models": {
        "code": "deepseek-coder:6.7b",
        "rag": "gemma3:4b",
        "docs": "phi3:mini",
        "vision": "qwen3-vl:8b"
      },
      "installation": "native (not Docker)",
      "port": 11434,
      "when_to_use": {
        "exploration": "mlx rag <project> 'how does X work?' - Use when exploring unfamiliar code",
        "code_review": "mlx review <file> - Run after writing significant code",
        "error_analysis": "mlx error - When user shares stack traces",
        "quick_queries": "mlx q <project> 'question' - For file/function lookups"
      },
      "when_to_skip": "During mid-implementation when files are already loaded in context",
      "tools": {
        "mlx q <project> <question>": "Quick hybrid search queries",
        "mlx rag <project> <question>": "RAG-powered answers with file context",
        "mlx similar <project> <file>": "Find related files",
        "mlx review <file>": "Code review for bugs/security issues",
        "mlx error": "Analyze stack traces/errors",
        "mlx commit": "Generate commit messages",
        "mlx pr [base]": "Generate PR descriptions",
        "doc_query <question>": "Query personal documents via AnythingLLM RAG (workspace: TERRA)"
      },
      "priority": "critical",
      "addedOn": "2025-12-31",
      "lastUpdated": "2026-01-10"
    },
    {
      "topic": "gemini cli integration",
      "rule": "Gemini CLI is available as a complementary AI tool. SUGGEST using it for: (1) parallel subtasks, (2) research/comparison tasks, (3) second opinions. Always ASK before invoking.",
      "usage": {
        "oneShot": "gemini \"prompt\" 2>/dev/null",
        "withModel": "gemini -m gemini-2.5-pro \"prompt\" 2>/dev/null"
      },
      "whenToSuggest": [
        "Complex debugging where a second perspective helps",
        "Research tasks that can run in parallel",
        "Tasks involving Google-specific knowledge (Firebase, GCP, Android)"
      ],
      "priority": "medium",
      "addedOn": "2025-12-19",
      "lastUpdated": "2026-01-10"
    }
  ],

  "patterns": [
    {
      "pattern": "Docker vs Native decision tree",
      "rule": "GPU workloads (LLM, ML) → Native. Isolated services (databases, web apps) → Docker.",
      "addedOn": "2026-01-13"
    },
    {
      "pattern": "Consolidate redundant services",
      "rule": "Before adding new tools, check if existing tools already cover the use case",
      "addedOn": "2026-01-13"
    },
    {
      "pattern": "Document infrastructure decisions",
      "rule": "Record system-wide decisions in ~/.claude-dash/global/infrastructure.json with reason, impact, and reversibility",
      "addedOn": "2026-01-13"
    },
    {
      "pattern": "Common tool workflow",
      "rule": "Typical sequence: Read → Edit → Bash (verify). Start with reading to understand context before editing.",
      "source": "transcript_analysis",
      "addedOn": "2026-01-16"
    },
    {
      "pattern": "Track app versions carefully",
      "rule": "For GYST mobile builds, always verify version and build numbers before EAS build. Past wasted resources from version mismatches.",
      "projects": ["gyst"],
      "priority": "high",
      "source": "correction_history",
      "addedOn": "2026-01-16"
    },
    {
      "pattern": "Request type distribution",
      "rule": "Most requests are add/create (40%), then search/find (22%), bug fixes (20%). Optimize for creation workflows.",
      "source": "transcript_analysis",
      "addedOn": "2026-01-16"
    }
  ],

  "learned_from_transcripts": {
    "extracted_on": "2026-01-16",
    "work_hours": {
      "peak_afternoon": "14:00-18:00",
      "peak_night": "22:00-02:00",
      "low_activity": "03:00-12:00"
    },
    "most_used_tools": ["Bash", "Edit", "Read", "TodoWrite"],
    "common_file_types": [".py", ".js", ".json", ".sh"],
    "most_active_projects": [".claude-dash", "WardrobeApp", "jamelna"]
  }
}
