{
  "version": "1.0",
  "lastUpdated": "2026-01-13T23:30:00Z",

  "use": [
    {
      "item": "Native apps over Docker for GPU-intensive workloads",
      "reason": "Metal GPU acceleration not available in Docker on Mac",
      "examples": ["Ollama", "ML inference"],
      "addedOn": "2026-01-13"
    },
    {
      "item": "AnythingLLM for local AI chat and document RAG",
      "reason": "Single tool for chat + RAG, API accessible via claude-dash gateway",
      "addedOn": "2026-01-13"
    },
    {
      "item": "qwen2.5:7b for reasoning tasks",
      "reason": "Better quality than 3b models, good multilingual support",
      "addedOn": "2026-01-13"
    }
  ],

  "avoid": [
    {
      "item": "Running LLMs in Docker on Mac",
      "reason": "No Metal GPU = 40x slower inference",
      "alternative": "Use native Ollama installation",
      "addedOn": "2026-01-13"
    },
    {
      "item": "Redundant tools with overlapping functionality",
      "reason": "Simplicity, less maintenance, fewer resources",
      "example": "Removed Open WebUI (AnythingLLM covers same use cases)",
      "addedOn": "2026-01-13"
    }
  ],

  "conventions": [
    {
      "topic": "claude workflow",
      "rule": "Use MCP memory tools (memory_query, memory_search, memory_functions) for instant lookups. Use MLX CLI (mlx rag, mlx review) for reasoning tasks that need local AI.",
      "priority": "critical",
      "addedOn": "2025-12-15",
      "lastUpdated": "2026-01-10"
    },
    {
      "topic": "model selection for agents",
      "rule": "Use gradient approach: MLX local ($0) -> Haiku ($) -> Sonnet ($$) -> Opus ($$$$). Reserve Opus for genuinely complex architecture only (~25% of planning). Use Sonnet for 75% of planning and ALL implementation.",
      "decision_tree": {
        "planning": {
          "complex": "Opus - Greenfield architecture, multiple non-obvious approaches, high reversal cost, cross-cutting concerns",
          "standard": "Sonnet - Following patterns, clear approach, well-understood problems, refactoring (75% of cases)"
        },
        "implementation": "Sonnet - ALWAYS use Sonnet, even for Opus-planned features",
        "exploration": "Haiku - Fast file/code exploration",
        "quick_tasks": "Haiku - Formatting, simple refactors, debug logging, documentation"
      },
      "examples": [
        "Task(model: 'opus') ONLY for genuinely complex architecture (e.g., 'design offline-first sync system')",
        "Task(model: 'sonnet') for standard planning (e.g., 'plan user profile screen')",
        "Task(model: 'sonnet') for ALL implementation/coding",
        "Task(model: 'haiku') for exploration, simple tasks"
      ],
      "priority": "critical",
      "addedOn": "2025-12-18",
      "lastUpdated": "2026-01-10"
    },
    {
      "topic": "ollama local ai tools",
      "rule": "Use NATIVE Ollama (Metal GPU) with MLX tools PROACTIVELY to save tokens. Model: qwen2.5:7b. Use for RAG queries, code review, error analysis. Also use doc_query for personal document searches.",
      "model": "qwen2.5:7b",
      "installation": "native (not Docker)",
      "port": 11434,
      "when_to_use": {
        "exploration": "mlx rag <project> 'how does X work?' - Use when exploring unfamiliar code",
        "code_review": "mlx review <file> - Run after writing significant code",
        "error_analysis": "mlx error - When user shares stack traces",
        "quick_queries": "mlx q <project> 'question' - For file/function lookups"
      },
      "when_to_skip": "During mid-implementation when files are already loaded in context",
      "tools": {
        "mlx q <project> <question>": "Quick hybrid search queries",
        "mlx rag <project> <question>": "RAG-powered answers with file context",
        "mlx similar <project> <file>": "Find related files",
        "mlx review <file>": "Code review for bugs/security issues",
        "mlx error": "Analyze stack traces/errors",
        "mlx commit": "Generate commit messages",
        "mlx pr [base]": "Generate PR descriptions",
        "doc_query <question>": "Query personal documents via AnythingLLM RAG (workspace: TERRA)"
      },
      "priority": "critical",
      "addedOn": "2025-12-31",
      "lastUpdated": "2026-01-10"
    },
    {
      "topic": "gemini cli integration",
      "rule": "Gemini CLI is available as a complementary AI tool. SUGGEST using it for: (1) parallel subtasks, (2) research/comparison tasks, (3) second opinions. Always ASK before invoking.",
      "usage": {
        "oneShot": "gemini \"prompt\" 2>/dev/null",
        "withModel": "gemini -m gemini-2.5-pro \"prompt\" 2>/dev/null"
      },
      "whenToSuggest": [
        "Complex debugging where a second perspective helps",
        "Research tasks that can run in parallel",
        "Tasks involving Google-specific knowledge (Firebase, GCP, Android)"
      ],
      "priority": "medium",
      "addedOn": "2025-12-19",
      "lastUpdated": "2026-01-10"
    }
  ],

  "patterns": [
    {
      "pattern": "Docker vs Native decision tree",
      "rule": "GPU workloads (LLM, ML) → Native. Isolated services (databases, web apps) → Docker.",
      "addedOn": "2026-01-13"
    },
    {
      "pattern": "Consolidate redundant services",
      "rule": "Before adding new tools, check if existing tools already cover the use case",
      "addedOn": "2026-01-13"
    },
    {
      "pattern": "Document infrastructure decisions",
      "rule": "Record system-wide decisions in ~/.claude-dash/global/infrastructure.json with reason, impact, and reversibility",
      "addedOn": "2026-01-13"
    }
  ]
}
