{
  "version": "1.0",
  "lastUpdated": "2026-01-13T23:30:00Z",
  "description": "System-wide infrastructure decisions and configurations",

  "decisions": [
    {
      "id": "ollama-native",
      "date": "2026-01-13",
      "decision": "Run Ollama natively on Mac instead of Docker",
      "reason": "Metal GPU acceleration provides ~40x performance improvement (28 TPS vs ~5 TPS in Docker)",
      "impact": "All services using Ollama now connect via host.docker.internal:11434",
      "reversible": true
    },
    {
      "id": "removed-anythingllm",
      "date": "2026-01-19",
      "decision": "Removed AnythingLLM - use claude-dash memory_query for RAG",
      "reason": "claude-dash gateway already provides RAG via memory_query, AnythingLLM was redundant",
      "impact": "Simplified stack, one less service to manage",
      "reversible": true
    }
  ],

  "services": {
    "ollama": {
      "type": "native",
      "path": "/opt/homebrew/bin/ollama",
      "port": 11434,
      "models": ["qwen2.5-coder:7b", "gemma3:4b-it-qat", "nomic-embed-text"],
      "gpu": "Metal"
    },
    "claude-dash-gateway": {
      "type": "native",
      "port": 3847,
      "tools": ["memory_query", "memory_search", "memory_functions", "memory_similar"]
    },
    "memory-dashboard": {
      "type": "docker",
      "port": 3333
    }
  },

  "docker_compose_notes": {
    "ollama": "DISABLED - using native for Metal GPU",
    "open-webui": "REMOVED - no longer needed",
    "anythingllm": "REMOVED - using claude-dash memory_query for RAG"
  }
}
